{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Assistants - File Search\n",
    "\n",
    "The Azure OpenAI Assistants file search tool automatically parses and chunks your documents, creates and stores the embeddings, and use both vector and keyword search to retrieve relevant content to answer user queries.\n",
    "\n",
    "File Formats: https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/file-search?tabs=python\n",
    "\n",
    "OpenAI FAQ: https://help.openai.com/en/articles/8550641-assistants-api-v2-faq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Create a client\n",
    "client = AzureOpenAI(\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key\n",
    ")\n",
    "\n",
    "# Create a vector store\n",
    "vector_store = client.beta.vector_stores.create(name=\"Nasa Books\")\n",
    "\n",
    "# Specify the folder containing the files\n",
    "folder_path = \"../Data/nasabooks/\"\n",
    "\n",
    "# Get all file paths in the folder\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path)]\n",
    "\n",
    "# Open file streams\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)\n",
    "print(vector_store.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat citations with the proper filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_citations(content_block):\n",
    "    # Extract the annotations\n",
    "    annotations = content_block.text.annotations\n",
    "    \n",
    "    # Original response\n",
    "    paragraph = content_block.text.value\n",
    "\n",
    "    # Dictionary to store key-value pairs of text and filename\n",
    "    text_filename_pairs = {}\n",
    "\n",
    "    # Iterate over the annotations and extract the relevant information\n",
    "    for annotation in annotations:\n",
    "        file_id = annotation.file_citation.file_id\n",
    "        text = annotation.text\n",
    "        cited_file = client.files.retrieve(file_id)\n",
    "        filename = cited_file.filename\n",
    "\n",
    "        if text not in text_filename_pairs:\n",
    "            text_filename_pairs[text] = []\n",
    "        text_filename_pairs[text].append(filename)\n",
    "\n",
    "    # Replace the citation texts with their corresponding filenames prefixed with \" Source: \"\n",
    "    for text, filenames in text_filename_pairs.items():\n",
    "        sources = \" Source: \" + \", \".join(filenames)\n",
    "        paragraph = paragraph.replace(text, sources)\n",
    "\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1-2:\n",
    "1. Create an Assistant\n",
    "2. Create a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Nasa books Assistant\",\n",
    "  instructions=\"\"\"\n",
    "  You are a assistant that provides information. \n",
    "   You will answer questions based on files provided to you about information in a NASA Book. \n",
    "   You will not provide answers outside of those files.\n",
    "  \"\"\",\n",
    "  model=azure_openai_deployment,\n",
    "  tools=[{\"type\":\"file_search\"}],\n",
    "  tool_resources={\"file_search\":{\"vector_store_ids\":[vector_store.id]}},\n",
    "  temperature=1,\n",
    "  top_p=1\n",
    ")\n",
    "\n",
    "# Step 2: Create thread\n",
    "thread = client.beta.threads.create()\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3-6: Helper Function\n",
    "3. Add a message to the thread\n",
    "4. Run the Assistant\n",
    "5. Check the Run Status\n",
    "6. Display the Assistant's Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_assistant(user_question):\n",
    "  # Step 3: Add a message to the thread\n",
    "  messages = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=user_question\n",
    "  )\n",
    "\n",
    "  # Step 4: Run the Assistant\n",
    "  run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    "  )\n",
    "\n",
    "  # Step 5: Check the Run Status\n",
    "  # Looping until the run completes or fails\n",
    "  while run.status in ['queued', 'in_progress', 'cancelling']:\n",
    "    time.sleep(1)\n",
    "    run = client.beta.threads.runs.retrieve(\n",
    "      thread_id=thread.id,\n",
    "      run_id=run.id\n",
    "    )\n",
    "\n",
    "    #display run status\n",
    "    print(run.status)\n",
    "\n",
    "    if run.status == 'completed':\n",
    "      messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "      # Step 6: Display the Assistant's Response\n",
    "      content_block = messages.data[0].content[0]\n",
    "      annotations = content_block.text.annotations\n",
    "      if annotations is None:\n",
    "        value = content_block.text.value\n",
    "        print(value)\n",
    "      else:\n",
    "        print(reformat_citations(content_block))\n",
    "    \n",
    "    elif run.status == 'requires_action':\n",
    "      pass\n",
    "    \n",
    "    elif run.status == \"failed\":\n",
    "        print(run.last_error)\n",
    "        \n",
    "    else:\n",
    "      pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question =\"\"\"How did the wide floodplains in Queensland originate?\"\"\"\n",
    "run_assistant(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question =\"\"\"What forms the Lower Amazon River?\"\"\"\n",
    "run_assistant(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.beta.assistants.delete(assistant.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
