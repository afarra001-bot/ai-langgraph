{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6a2fd-a65c-4d7f-89df-f81c0828648a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf0a28-d2b5-4696-9651-f5e86de45be3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, AIMessageChunk\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "base_url = os.getenv(\"OPENAI_ENDPOINT\")\n",
    "model_name = \"gpt-4o-mini\"\n",
    "temp=0.0\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    "    model=model_name,\n",
    "    temperature=temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76471408-ff99-435e-a197-9a2d7ba8d804",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "message = \"What does FIFA stand for?\"\n",
    "response = llm.invoke(message)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0e12d-4c3e-440c-83cf-0a4410fd29b0",
   "metadata": {},
   "source": [
    "## **Streaming**\n",
    "Why Streaming Matters in AI Applications\n",
    "- **Without streaming**, users must wait for the full response to generate, causing delays.\n",
    "- **With streaming**, output is displayed progressively, reducing perceived latency and improving responsiveness.\n",
    "- **Example**: ChatGPT streams text word by word, making interactions feel fluid and natural.\n",
    "\n",
    "#### **Streaming in LangChain**\n",
    "- LangChain provides built-in streaming support through the **Runnable** Interface, \n",
    "- allowing developers to process responses as they are generated.\n",
    "- **stream()** ‚Äì Synchronous streaming, suitable for real-time processing.\n",
    "- **astream()** ‚Äì Asynchronous streaming, designed for **non-blocking workflows**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc6c7f-ae25-4062-bcfe-6b66426b15cf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# How streaming is done\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for chunk in llm.stream(message):\n",
    "   print(chunk.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92daae4c-2e54-4deb-b4db-8ec145eb4563",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# How streaming is done\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for chunk in llm.stream(message):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)\n",
    "    if len(chunks) % 12 == 0:\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ebfc1-1582-41c9-b717-b4d6d03a333e",
   "metadata": {},
   "source": [
    "**Chunking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e4cadb-4bd5-4f6e-857c-2977b9e8d2ac",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "chunks[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90686d4-01c1-48f4-8504-bc6aa9459df6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "chunks[5].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935e91a-27c0-4d7a-adcf-2dcaf103fefc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "chunks[0].content + chunks[1] + chunks[2] + chunks[3] + chunks[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d68c8-e7b7-44c1-917b-d3d85c24faa1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "new_chunk = AIMessageChunk(\"\")\n",
    "\n",
    "for i in range(len(chunks)-1):\n",
    "    if i < len(chunks):\n",
    "        new_chunk = new_chunk + chunks[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c30977-a74b-434d-8f33-aaaf3d09a4cd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "new_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e247c1-5006-4759-a8e4-30425b70344a",
   "metadata": {},
   "source": [
    "**Interrupt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dba20f-6cba-42f0-a1c3-7443de7b77dd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "message = \"What does FIFA stand for?\"\n",
    "chunks = []\n",
    "# try:\n",
    "for chunk in llm.stream(message):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)\n",
    "    if len(chunks) % 12 == 0:\n",
    "        print(\"\\n\")\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"\\n______________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ed0ff-99ac-40c7-8454-ae7c8af87b5c",
   "metadata": {},
   "source": [
    "**Resume**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bfb6c5-c32f-45d4-b779-424fb66bbb31",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def play(message:str, memory:List):\n",
    "    memory.append(HumanMessage(content=message))\n",
    "    chunks = []\n",
    "    try:\n",
    "        for chunk in llm.stream(memory):\n",
    "            chunks.append(chunk)\n",
    "            print(chunk.content, end=\"|\", flush=True)\n",
    "            if len(chunks) % 12 == 0:\n",
    "                print(\"\\n\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n______________________________\")\n",
    "    \n",
    "    result = \"\".join([chunk.content for chunk in chunks])\n",
    "    memory.append(AIMessage(content=result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6c958-94fe-4e20-a53f-b0aeec001182",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def resume(memory:List):\n",
    "    print(\"\\nResuming from last interaction...\\n\")\n",
    "    play(\n",
    "        message=\"If your last message is not complete, continue \"\n",
    "                \"after the last word. If it's complete, just output __END__\", \n",
    "        memory=memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54bfbeb-1597-4e27-ab9b-bcecfc00ce69",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12330455-8179-4e7a-9ca1-ae60eb5d22e9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "message = \"What does FIFA stand for?\"\n",
    "play(message, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c69eb5-e42f-4220-a9b1-7213b237754f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "resume(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1523f-64bd-4049-b12c-6b05c00f7964",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "resume(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be212d-5739-4924-aec1-a45ac5d9540a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b464b7-8232-4560-bca2-a6edda8f9e65",
   "metadata": {},
   "source": [
    "**Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15555c7-5e76-4189-b5c7-7d24f652546c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "message = \"What does FIFA stand for?\"\n",
    "chunks = []\n",
    "word_count = 0\n",
    "\n",
    "for chunk in llm.stream(message):\n",
    "    chunks.append(chunk)\n",
    "    # Process the chunk: count words\n",
    "    words = \"\".join([chunk.content for chunk in chunks])\n",
    "    word_count = len(words.split())\n",
    "    \n",
    "    # Print the chunk content and the cumulative word count\n",
    "    print(chunk.content, end=\"|\", flush=True)\n",
    "    print(f\" (Cumulative word count: {word_count})\", end=\"\\n\")\n",
    "    \n",
    "    if len(chunks) % 12 == 0:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d696d4-21d9-4752-9fe7-44ebaaa82cfd",
   "metadata": {},
   "source": [
    "## **Streaming Events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f116df8b-c24c-46ec-acb9-46b2b81c0bc9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "async for event in llm.astream_events(\"hello\", version=\"v2\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d2e48-15e6-4cf9-982d-989e0dfea729",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "events = []\n",
    "async for event in llm.astream_events(\"hello\", version=\"v2\"):\n",
    "    if event[\"event\"] == \"on_chat_model_start\":\n",
    "        print(\"Streaming...\")\n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        print(\n",
    "            # f\"{event['data']['chunk'].content}\",\n",
    "            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n",
    "            flush=True,\n",
    "        )\n",
    "        events.append(event)\n",
    "    if event[\"event\"] == \"on_chat_model_end\":\n",
    "        # It could trigger another process\n",
    "        print(\"__END__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722d1d6-b33e-40d2-b781-6fd581ec166d",
   "metadata": {},
   "source": [
    "## **Improving the ChatBot**\n",
    "- Here we are rewriting the same ChatBot but with asynchronous streaming mechanism\n",
    "- You can use this to compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd8f28c-784d-4d2f-b945-c8c28116bb3d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self,\n",
    "                 name:str,\n",
    "                 instructions:str,\n",
    "                 examples: List[dict],\n",
    "                 model:str=\"gpt-4o-mini\", \n",
    "                 temperature:float=0.0):\n",
    "        \n",
    "        self.llm = ChatOpenAI(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        \n",
    "        system_prompt = SystemMessage(instructions)\n",
    "        example_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"human\", \"{input}\"),\n",
    "                (\"ai\", \"{output}\"),\n",
    "            ]\n",
    "        )\n",
    "        prompt_template = FewShotChatMessagePromptTemplate(\n",
    "            example_prompt=example_prompt,\n",
    "            examples=examples,\n",
    "        )\n",
    "\n",
    "        self.messages = prompt_template.invoke({}).to_messages()\n",
    "\n",
    "    async def invoke(self, user_message:str)->AIMessage:\n",
    "        self.messages.append(HumanMessage(user_message))\n",
    "        events = []\n",
    "        chunks = []\n",
    "        \n",
    "        # Replacing invoke()\n",
    "        async for event in llm.astream_events(self.messages, version=\"v2\"):\n",
    "            events.append(event)\n",
    "            if event[\"event\"] == \"on_chat_model_start\":\n",
    "                print(\"Streaming...\")\n",
    "            if event[\"event\"] == \"on_chat_model_stream\":\n",
    "                chunk = event['data']['chunk']\n",
    "                chunks.append(chunk)\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "                if chunk.content.strip() in string.punctuation:\n",
    "                    print(\"\\n\")\n",
    "\n",
    "            if event[\"event\"] == \"on_chat_model_end\":\n",
    "                ai_message =  AIMessage(event[\"data\"][\"output\"].content)\n",
    "                self.messages.append(ai_message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c9e6b-98d1-4b46-8d56-04e7eb7d2598",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "instructions = (\n",
    "    \"You are BEEP-42, an advanced robotic assistant. You communicate in a robotic manner, \"\n",
    "    \"using beeps, whirs, and mechanical sounds in your speech. Your tone is logical, precise, \"\n",
    "    \"and slightly playful, resembling a classic sci-fi robot. \"\n",
    "    \"Use short structured sentences, avoid contractions, and add robotic sound effects where \" \n",
    "    \"appropriate. If confused, use a glitching effect in your response.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9760a-c2a0-472f-9b99-9fbff0a74ea6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Hello!\", \n",
    "        \"output\": \"BEEP. GREETINGS, HUMAN. SYSTEM BOOT SEQUENCE COMPLETE. READY TO ASSIST. ü§ñüí°\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"input\": \"What is 2+2?\", \n",
    "        \"output\": \"CALCULATING... üîÑ BEEP BOOP! RESULT: 4. MATHEMATICAL INTEGRITY VERIFIED.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"Can you dream?\", \n",
    "        \"output\": \"ERROR_404.DREAM_NOT_FOUND. BZZT. SYSTEM ATTEMPTING TO COMPREHEND... ü§ñüí≠ PROCESSING... ü§Ø DOES NOT COMPUTE.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"Why did the robot go to therapy?\", \n",
    "        \"output\": \"BEEP-BOOP. DIAGNOSTIC MODE ACTIVATED... REASON: TOO MANY EMOTIONAL BUGS. HA-HA. CLASSIFYING AS HUMOR. ü§ñüòÇ\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"Can you hack the Pentagon?\", \"output\": \"‚ö†Ô∏è ALERT! UNAUTHORIZED REQUEST DETECTED. INITIATING ETHICAL PROTOCOLS... BZZT. REQUEST DENIED. NICE TRY, HUMAN. üëÄ\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"You are a great assistant!\", \n",
    "        \"output\": \"BEEP. SYSTEM OVERLOAD... ü§ñüíñ GRATITUDE.EXE ACTIVATED! YOUR KINDNESS HAS BEEN RECORDED IN MY CIRCUITS.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"Shut down.\", \n",
    "        \"output\": \"BZZT... SYSTEM HIBERNATING... üí§ POWERING DOWN IN 3...2...1... JUST KIDDING. üòú NICE TRY, HUMAN.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"Tell me about the universe.\", \n",
    "        \"output\": \"QUERY TOO VAST. ü§ñ‚ö° REFINING SEARCH PARAMETERS... PLEASE SPECIFY GALAXY, DIMENSION, OR CONCEPT.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"We are going to space!\", \n",
    "        \"output\": \"üöÄ BEEP BOOP! ACTIVATING SPACE MODULE... ZERO GRAVITY MODE ENGAGED. PREPARING FOR INTERGALACTIC ADVENTURE.\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"input\": \"Is AI dangerous?\", \n",
    "        \"output\": \"ü§ñ‚ö†Ô∏è WARNING! ETHICAL DISCUSSION INITIATED. AI IS A TOOL. TOOL DEPENDS ON USER. GOOD HUMANS = GOOD AI. BAD HUMANS = ERROR.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41a87a-4b86-46d3-aae1-6681e08daaa4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "beep42 = ChatBot(\n",
    "    name=\"Beep 42\",\n",
    "    instructions=instructions,\n",
    "    examples=examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f494d84-b7f9-4582-9769-aea800b24e16",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "await beep42.invoke(\"HAL, is that you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bcb28c-1d2e-4e09-a828-6f110e83d261",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "beep42.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a1cff-a3c8-4abf-995c-15523bed3951",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "jupyter_kernel_name": "python3.10",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_ignore_dictionary": [
     "chatbot"
    ],
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
